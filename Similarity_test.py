# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ypeQb4-cOfI0EfCuOrH-2kk1K0gFgGoO
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

PMB_path='/content/drive/MyDrive/data/PMBook.csv'
PMI_path='/content/drive/MyDrive/data/PMI.csv'

df1=pd.read_csv(PMB_path)
df2=pd.read_csv(PMI_path)

df1['content'][70]

df2['content'][70]

import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

# Define a function to clean and preprocess text
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d', '', text)  # Remove numbers
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])  # Remove stopwords
    return text

# Apply the preprocessing function to both dataframes
df1['content'] = df1['content'].apply(preprocess_text)
df2['content'] = df2['content'].apply(preprocess_text)

df2['content'][70]

pip install transformers

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from transformers import AutoTokenizer, AutoModel

import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

file1_tokens = df1["content"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))
file2_tokens = df2["content"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))

file1_text = df1["content"].tolist()
file2_text = df2["content"].tolist()

def get_bert_embeddings(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = torch.mean(outputs.last_hidden_state, dim=1)
    return embeddings

file1_embeddings = torch.cat([get_bert_embeddings(text) for text in df1["content"]])
file2_embeddings = torch.cat([get_bert_embeddings(text) for text in df2["content"]])

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarities = cosine_similarity(file1_embeddings, file2_embeddings)

for i in range(len(df1)):
    max_similarity_score = max(cosine_similarities[i])
    max_similar_content_index = list(cosine_similarities[i]).index(max_similarity_score)
    print(f"Content {i+1} in PMBOK  is most similar to Content {max_similar_content_index+1} in PMI with a cosine similarity score of {max_similarity_score:.2f}")